{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Malignant comment model training.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4gLZiTf-yW0"
      },
      "source": [
        "#Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfdeyJSoD5lT",
        "outputId": "4f37ba01-d8fc-43bc-b912-1b0e4b3a4293"
      },
      "source": [
        "import pandas as pd\n",
        "data=pd.read_csv('/content/drive/MyDrive/comments/new_master_data.csv')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (4,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhnLhH0n_Wo7"
      },
      "source": [
        "**This new data contains the augmented data as well which i have generated by using transformers models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSJPpwfaU5yh",
        "outputId": "1375f6b3-9f5a-4bcf-c445-80937a2c11aa"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import re"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWnQh1_eD_CR",
        "outputId": "c4fdadd2-1f04-44ab-8320-db9261bdd222"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsU6gxA2HNs2"
      },
      "source": [
        "normal_comments=data[(((((data['malignant']!=1) &(data['highly_malignant']!=1) & (data['rude']!=1) & (data['threat']!=1) & (data['abuse']!=1) & (data['loathe']!=1)))))]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SdStQNe-3Wm"
      },
      "source": [
        "as i have a large no of dataset and my system is not able to handle that much thats why i am selecting the normal comments there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-sXoVKiHQbm"
      },
      "source": [
        "normal_comments_data=normal_comments.sample(n=100000)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX98e8C9_BDm"
      },
      "source": [
        "here i have selected 100000 rows as a normal data that have only 0 value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aaHIpMiHfG6"
      },
      "source": [
        "malignant=data[data['malignant']==1]\n",
        "highly_malignant=data[data['highly_malignant']==1]\n",
        "rude=data[data['rude']==1]\n",
        "threat=data[data['threat']==1]\n",
        "abuse=data[data['abuse']==1]\n",
        "loathe=data[data['loathe']==1]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3h5a4geHrso"
      },
      "source": [
        "frames=[normal_comments_data,malignant,rude,threat,abuse,loathe]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1VJiBP-UMqc"
      },
      "source": [
        "new_data=pd.concat(frames)\n",
        "new_data.drop(['Unnamed: 0','level_0','index'],axis=1,inplace=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxqjPAFA_Llm"
      },
      "source": [
        "Here i have concat the all labels values these value have the augmented data as well"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FOE4de1URuB"
      },
      "source": [
        "data = new_data.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "id": "4UrYhk6KGqYl",
        "outputId": "dffa512f-5371-4612-b3f1-1d685123eba2"
      },
      "source": [
        "data"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>malignant</th>\n",
              "      <th>highly_malignant</th>\n",
              "      <th>rude</th>\n",
              "      <th>threat</th>\n",
              "      <th>abuse</th>\n",
              "      <th>loathe</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>clean_comment_text</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>Labels</th>\n",
              "      <th>encoded_cat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>I also make mistakes, but I'm not a fucking va...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94247.0</td>\n",
              "      <td>also make mistake fucking vandal told hate cal...</td>\n",
              "      <td>94247.0</td>\n",
              "      <td>malignant,rude,abuse</td>\n",
              "      <td>23.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>That's next day, so I might as well see Main P...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>81695.0</td>\n",
              "      <td>next day might well see main page</td>\n",
              "      <td>81695.0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>That's unquestionably unfair. I've seen loads ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>105454.0</td>\n",
              "      <td>unquestionably unfair seen load screenshots pe...</td>\n",
              "      <td>105454.0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>u motherfukkin bitch i want to rape you smelly...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>29968.0</td>\n",
              "      <td>u motherfukkin bitch want rape smelly whore st...</td>\n",
              "      <td>29968.0</td>\n",
              "      <td>malignant,highly_malignant,rude,threat,abuse,l...</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>It's not even on television yet.</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>115596.0</td>\n",
              "      <td>even television yet</td>\n",
              "      <td>115596.0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178212</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>12194.0</td>\n",
              "      <td>daedalus nothing filthy n gger cyberstalker si...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178213</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11519.0</td>\n",
              "      <td>I'm wondering if faggot large participate sexu...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178214</th>\n",
              "      <td>0</td>\n",
              "      <td>\"\\n\\n further move needed on reviewed article ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36206.0</td>\n",
              "      <td>move needed reviewed article hi thanks reviewi...</td>\n",
              "      <td>36206.0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178215</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>12452.0</td>\n",
              "      <td>Is it possible that people god behold nice thi...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178216</th>\n",
              "      <td>0</td>\n",
              "      <td>Stop the vandalism.  11:09, 7 Oct 2004 (UTC)</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>123397.0</td>\n",
              "      <td>stop vandalism oct utc</td>\n",
              "      <td>123397.0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178217 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ... encoded_cat\n",
              "0        0  ...        23.0\n",
              "1        0  ...         0.0\n",
              "2        0  ...         0.0\n",
              "3        0  ...        17.0\n",
              "4        0  ...         0.0\n",
              "...     ..  ...         ...\n",
              "178212   0  ...         0.0\n",
              "178213   0  ...         0.0\n",
              "178214   0  ...         0.0\n",
              "178215   0  ...         0.0\n",
              "178216   0  ...         0.0\n",
              "\n",
              "[178217 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDkFLwTsEKWK"
      },
      "source": [
        "#wnl=WordNetLemmatizer()\n",
        "#corpus=[]\n",
        "\n",
        "\"\"\"for i in range(len(data)):\n",
        "  review=re.sub('[^a-zA-Z]',' ',data['clean_comment_text'][i])\n",
        "  review=review.lower()\n",
        "  review=review.split()\n",
        "  review=[wnl.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
        "  review=' '.join(review)\n",
        "  corpus.append(review)\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-duWqM2ZEKTE"
      },
      "source": [
        "#data['clean_comment_text']=corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywd8HqRB_gqB"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vec=TfidfVectorizer(ngram_range=(1,2),\n",
        "                          min_df=2,\n",
        "                          max_features=15000)\n",
        "tfidf_vec.fit(data['clean_comment_text'])\n",
        "\n",
        "# trasform train and test\n",
        "train_tfidf = tfidf_vec.transform(data['clean_comment_text'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sC14JPs_lGv"
      },
      "source": [
        "Here i am using tfidf for converting my text data into numeric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RG2kdGUFtZq"
      },
      "source": [
        "from sklearn.model_selection import train_test_split,cross_val_score,StratifiedKFold,GridSearchCV\n",
        "from sklearn.metrics import accuracy_score,classification_report,f1_score,roc_auc_score"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_01AW_vPGl7r"
      },
      "source": [
        "list_classes = ['malignant','highly_malignant','rude','threat','abuse','loathe']\n",
        "train_y = data[list_classes].values\n",
        "X_train,X_test,y_train,y_test=train_test_split(train_tfidf,train_y,test_size=0.3,random_state=42)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qu2E8QPEcHg"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuefMbfmD-kZ"
      },
      "source": [
        "models={\"RandomForestClassifier\":RandomForestClassifier(),\n",
        "        \"DecisionTreeClassifier\":DecisionTreeClassifier(),\n",
        "        \"ExtraTreesClassifier\":ExtraTreesClassifier(),\n",
        "        \"KNeighborsClassifier\":KNeighborsClassifier()}"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nyV53cvEOdu",
        "outputId": "fd157686-3232-4593-e97b-cae5478527f1"
      },
      "source": [
        "for name,model in models.items():\n",
        "  print(\"****************************************************\",name,\"*********************************************************\")\n",
        "  print('\\n')\n",
        "  model.fit(X_train,y_train)\n",
        "  print(model)\n",
        "  y_pred=model.predict(X_test)\n",
        "  print('\\n')\n",
        "  acc=accuracy_score(y_test,y_pred)\n",
        "  print(\"Accuracy Score\",acc)\n",
        "  print('\\n')\n",
        "  F1_SCORE=f1_score(y_test,y_pred,average='micro')\n",
        "  print(\"F1 SCORE\",F1_SCORE)\n",
        "  report=classification_report(y_test,y_pred)\n",
        "  print('\\n')\n",
        "  print(\"CLASSIFICATION REPORT\")\n",
        "  print(\"\\n\")\n",
        "  print(report)\n",
        "  roc=roc_auc_score(y_test,y_pred)\n",
        "  print(\"ROC AUC SCORE\",roc)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************************************************** RandomForestClassifier *********************************************************\n",
            "\n",
            "\n",
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False)\n",
            "\n",
            "\n",
            "Accuracy Score 0.8168555717652339\n",
            "\n",
            "\n",
            "F1 SCORE 0.8069438358635976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "CLASSIFICATION REPORT\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.80      0.85      9702\n",
            "           1       0.95      0.87      0.91      1486\n",
            "           2       0.80      0.74      0.77      9628\n",
            "           3       0.89      0.88      0.89      4831\n",
            "           4       0.78      0.72      0.75      9123\n",
            "           5       0.85      0.76      0.80      5672\n",
            "\n",
            "   micro avg       0.84      0.77      0.81     40442\n",
            "   macro avg       0.86      0.79      0.83     40442\n",
            "weighted avg       0.84      0.77      0.81     40442\n",
            " samples avg       0.28      0.29      0.28     40442\n",
            "\n",
            "ROC AUC SCORE 0.8865871666285526\n",
            "**************************************************** DecisionTreeClassifier *********************************************************\n",
            "\n",
            "\n",
            "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
            "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=None, splitter='best')\n",
            "\n",
            "\n",
            "Accuracy Score 0.7904275614409157\n",
            "\n",
            "\n",
            "F1 SCORE 0.7758736642837573\n",
            "\n",
            "\n",
            "CLASSIFICATION REPORT\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.83      0.82      9702\n",
            "           1       0.89      0.87      0.88      1486\n",
            "           2       0.75      0.74      0.75      9628\n",
            "           3       0.84      0.82      0.83      4831\n",
            "           4       0.72      0.73      0.72      9123\n",
            "           5       0.80      0.72      0.76      5672\n",
            "\n",
            "   micro avg       0.78      0.77      0.78     40442\n",
            "   macro avg       0.80      0.78      0.79     40442\n",
            "weighted avg       0.78      0.77      0.78     40442\n",
            " samples avg       0.28      0.28      0.28     40442\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC AUC SCORE 0.8759874021557551\n",
            "**************************************************** ExtraTreesClassifier *********************************************************\n",
            "\n",
            "\n",
            "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
            "                     criterion='gini', max_depth=None, max_features='auto',\n",
            "                     max_leaf_nodes=None, max_samples=None,\n",
            "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                     min_samples_leaf=1, min_samples_split=2,\n",
            "                     min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
            "                     warm_start=False)\n",
            "\n",
            "\n",
            "Accuracy Score 0.8183518497736879\n",
            "\n",
            "\n",
            "F1 SCORE 0.8089456368478772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "CLASSIFICATION REPORT\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.78      0.84      9702\n",
            "           1       0.96      0.86      0.91      1486\n",
            "           2       0.81      0.73      0.77      9628\n",
            "           3       0.90      0.88      0.89      4831\n",
            "           4       0.79      0.71      0.75      9123\n",
            "           5       0.88      0.77      0.82      5672\n",
            "\n",
            "   micro avg       0.86      0.77      0.81     40442\n",
            "   macro avg       0.88      0.79      0.83     40442\n",
            "weighted avg       0.86      0.77      0.81     40442\n",
            " samples avg       0.28      0.28      0.28     40442\n",
            "\n",
            "ROC AUC SCORE 0.8848453876020378\n",
            "**************************************************** KNeighborsClassifier *********************************************************\n",
            "\n",
            "\n",
            "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
            "                     weights='uniform')\n",
            "\n",
            "\n",
            "Accuracy Score 0.6909250738787267\n",
            "\n",
            "\n",
            "F1 SCORE 0.6110477741585234\n",
            "\n",
            "\n",
            "CLASSIFICATION REPORT\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.38      0.47      9702\n",
            "           1       0.51      0.30      0.38      1486\n",
            "           2       0.64      0.52      0.57      9628\n",
            "           3       0.82      0.92      0.86      4831\n",
            "           4       0.63      0.47      0.54      9123\n",
            "           5       0.75      0.82      0.78      5672\n",
            "\n",
            "   micro avg       0.68      0.56      0.61     40442\n",
            "   macro avg       0.66      0.57      0.60     40442\n",
            "weighted avg       0.66      0.56      0.60     40442\n",
            " samples avg       0.25      0.25      0.24     40442\n",
            "\n",
            "ROC AUC SCORE 0.7643961640439844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bV6pPUZAIU5"
      },
      "source": [
        "**I will select extrastree as my best model**\n",
        "\n",
        "**Note=**I was training many model but that models was giving me error because this Y value have multilabel so just because of that i was not able to train my dataset on that models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN8RT2DlPaBn"
      },
      "source": [
        "Extra=ExtraTreesClassifier()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6pE95Bq2h34",
        "outputId": "c9ceb926-a1a6-4526-eb38-77c4b576b399",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Extra.fit(X_train,y_train)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
              "                     criterion='gini', max_depth=None, max_features='auto',\n",
              "                     max_leaf_nodes=None, max_samples=None,\n",
              "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                     min_samples_leaf=1, min_samples_split=2,\n",
              "                     min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
              "                     warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrnzrliyr1aq"
      },
      "source": [
        "pre=Extra.predict(X_test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMwWuCNv4YUO",
        "outputId": "e3b71618-beb5-45f6-93b1-b7b9895e5a10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "accuracy_score(y_test,pre)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8186698088504845"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fhwdi2twAjzY"
      },
      "source": [
        "#Laoding Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLIg7R104cSf"
      },
      "source": [
        "test=pd.read_csv('/content/drive/MyDrive/datatraine/test.csv')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLUp5iPzAmuI"
      },
      "source": [
        "#Cleaning Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8ATOmD04jdy"
      },
      "source": [
        "wnl=WordNetLemmatizer()\n",
        "corpus=[]\n",
        "\n",
        "for i in range(len(test)):\n",
        "  review=re.sub('[^a-zA-Z]',' ',test['comment_text'][i])\n",
        "  review=review.lower()\n",
        "  review=review.split()\n",
        "  review=[wnl.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
        "  review=' '.join(review)\n",
        "  corpus.append(review)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG2BHDVb4w2I"
      },
      "source": [
        "test['clean_comment_text']=corpus"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N09ilms4Ap62"
      },
      "source": [
        "#Feature Transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBw643t741v_"
      },
      "source": [
        "tfidf_vec = TfidfVectorizer(ngram_range=(1,2), \n",
        "                            min_df=2, \n",
        "                            max_features=15000)\n",
        "tfidf_vec.fit(test['clean_comment_text'])\n",
        "\n",
        "# trasform train and test\n",
        "test_tfidf = tfidf_vec.transform(test['clean_comment_text'])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MtnThFH4-pM"
      },
      "source": [
        "predictions=Extra.predict(test_tfidf)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wJXNh359vMI"
      },
      "source": [
        "pre_data=pd.DataFrame(predictions,columns=list_classes)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrN6IDo29e1M"
      },
      "source": [
        "test_data_comments=pd.concat([pre_data,test['clean_comment_text']],axis=1)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4cY6pURAtOI"
      },
      "source": [
        "#The result of prediction of malignant comment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4drMzL0-GKy",
        "outputId": "5efce62e-a1b6-4eb1-af49-3398156e97ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "test_data_comments[test_data_comments['malignant']==1]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>malignant</th>\n",
              "      <th>highly_malignant</th>\n",
              "      <th>rude</th>\n",
              "      <th>threat</th>\n",
              "      <th>abuse</th>\n",
              "      <th>loathe</th>\n",
              "      <th>clean_comment_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>adding new product list make sure relevant add...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>convinced blind documented possible call legal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>ridiculous unless good non disingenuous respon...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>could precis material instead downloading whol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>catdiffuse think best idea would check whatlin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153038</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>rate article definitely need work capitalizati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153061</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>sarah anyone edit page anyone nominate deletio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153106</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>youshit dick cock fuckshit dick cock fuck shit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153116</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>utc fyi currently edit conflict user bakasuprm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153159</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>totally agree stuff nothing long crap</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7466 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        malignant  ...                                 clean_comment_text\n",
              "14            1.0  ...  adding new product list make sure relevant add...\n",
              "41            1.0  ...  convinced blind documented possible call legal...\n",
              "75            1.0  ...  ridiculous unless good non disingenuous respon...\n",
              "100           1.0  ...  could precis material instead downloading whol...\n",
              "104           1.0  ...  catdiffuse think best idea would check whatlin...\n",
              "...           ...  ...                                                ...\n",
              "153038        1.0  ...  rate article definitely need work capitalizati...\n",
              "153061        1.0  ...  sarah anyone edit page anyone nominate deletio...\n",
              "153106        1.0  ...  youshit dick cock fuckshit dick cock fuck shit...\n",
              "153116        1.0  ...  utc fyi currently edit conflict user bakasuprm...\n",
              "153159        1.0  ...              totally agree stuff nothing long crap\n",
              "\n",
              "[7466 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wGllNjqA0Iz"
      },
      "source": [
        "#Highly_malignant comment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cziqu8Wf-Odo",
        "outputId": "e3e03f35-9ba8-408b-be13-3db2f8f42b17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "test_data_comments[test_data_comments['highly_malignant']==1]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>malignant</th>\n",
              "      <th>highly_malignant</th>\n",
              "      <th>rude</th>\n",
              "      <th>threat</th>\n",
              "      <th>abuse</th>\n",
              "      <th>loathe</th>\n",
              "      <th>clean_comment_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>62046</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>p user also editing people talk page</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67203</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>changing people talk page comment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       malignant  ...                    clean_comment_text\n",
              "62046        1.0  ...  p user also editing people talk page\n",
              "67203        1.0  ...     changing people talk page comment\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbFmuCxyA27I"
      },
      "source": [
        "#Abuse Comment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11O6yYW_-pGi",
        "outputId": "f471b8ed-dffc-4e83-a367-7b180cf13b14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "test_data_comments[test_data_comments['abuse']==1]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>malignant</th>\n",
              "      <th>highly_malignant</th>\n",
              "      <th>rude</th>\n",
              "      <th>threat</th>\n",
              "      <th>abuse</th>\n",
              "      <th>loathe</th>\n",
              "      <th>clean_comment_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>ridiculous unless good non disingenuous respon...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>avg plenty greek love king stop acting like gr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>recommendation point keep stuff main space</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>february utc certainly would ask include somet...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>related page fyi exists draft draft trumpler c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152654</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>page movement hello jk getting impression thar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152792</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>tripped keyboard oops tripped keyboard wrote t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153041</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>closed captioning read nigger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153090</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>dear sir fucking cunt rahm emanuel satannnnnnn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153159</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>totally agree stuff nothing long crap</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2745 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        malignant  ...                                 clean_comment_text\n",
              "75            1.0  ...  ridiculous unless good non disingenuous respon...\n",
              "106           0.0  ...  avg plenty greek love king stop acting like gr...\n",
              "288           1.0  ...         recommendation point keep stuff main space\n",
              "308           1.0  ...  february utc certainly would ask include somet...\n",
              "382           1.0  ...  related page fyi exists draft draft trumpler c...\n",
              "...           ...  ...                                                ...\n",
              "152654        1.0  ...  page movement hello jk getting impression thar...\n",
              "152792        1.0  ...  tripped keyboard oops tripped keyboard wrote t...\n",
              "153041        0.0  ...                      closed captioning read nigger\n",
              "153090        0.0  ...  dear sir fucking cunt rahm emanuel satannnnnnn...\n",
              "153159        1.0  ...              totally agree stuff nothing long crap\n",
              "\n",
              "[2745 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIyZDL2-A5AS"
      },
      "source": [
        "#Conclusion\n",
        "\n",
        "1-Did Cleaning of the Data\n",
        "\n",
        "2-Did EDA on the dataset\n",
        "\n",
        "3-Did Feature Engineering on data \n",
        "\n",
        "4-Did data augmentation because most of the labels was only having 500 or 1000 so my model was not able to learn properly\n",
        "\n",
        "5-Trained Multiple Models\n",
        "\n",
        "6-Select ExtraTreeClassifier as my Best Model"
      ]
    }
  ]
}